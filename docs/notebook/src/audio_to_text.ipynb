{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Import library`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from infy_gen_ai_fmwk.service.audio_to_text_service import AudioToTextService\n",
    "from infy_gen_ai_fmwk.data.config_data import AudioToTextConfigData, OpenAiLlmConfigData\n",
    "from infy_gen_ai_fmwk.data.request_data import AudioToTextRequestData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Configuration and Request Data Setup for audio to text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_config = OpenAiLlmConfigData(\n",
    "        completion_model=\"gpt-4\",\n",
    "        open_ai_url=os.environ['AZURE_OPENAI_API_BASE'],\n",
    "        open_ai_key=os.environ['AZURE_OPENAI_KEY_EAST_US'],\n",
    "        open_ai_version=os.environ['AZURE_OPENAI_API_VERSION'],\n",
    "        prompt_template=\"Generate detailed minutes of meeting from the below conversation and list the action items: {audio_text}\\nMOM:\"\n",
    "    )\n",
    "request_data = AudioToTextRequestData(\n",
    "        # Input file path should be updated by the user\n",
    "        file_path=\"\",\n",
    "        is_mom_required=True\n",
    "    )\n",
    "config_data = AudioToTextConfigData(\n",
    "        #Steps to download a model can be referred from README.md file.The model_path below needs to be updated by the user accordingly.\n",
    "        audio_converter_model_path=\"C:/MyProgramFiles/AI/models/whisper-base\",\n",
    "        compressed_audio_filepath=\"audio.flac\",\n",
    "        audio_chunck_length=30,\n",
    "        audio_Target_sr=16000,\n",
    "        audio_batch_size=3,\n",
    "        open_ai_config=open_ai_config\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Invoke audio to text service`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_service = AudioToTextService(config_data)\n",
    "result = audio_service.convert_to_text(request_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Display mom_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result.transcription_response_txt)\n",
    "print(result.mom_text )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
